{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "- chunk them out by 30 seconds to bootstrap and create more samples\n",
    "- try some cross validation \n",
    "- test for overfitting\n",
    "- more features \n",
    "    - get time signature from meta messages\n",
    "    - ~~stdev of velocity (instead of just average)~~\n",
    "    - create some manual cross variables with timing and key and time sig\n",
    "    - frequency domain features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer_class_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "train_midi_path = \"./Challenge_DataSet/PS1/\"\n",
    "test_midi_path = \"./Challenge_DataSet/PS2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_ps2 = test_midi_path+\"0.981087291054314_adj.mid\"\n",
    "file_path_ps1 = train_midi_path+\"Bach/Cello Suite 3_BWV1009_2217_cs3-1pre.mid\"\n",
    "x = extract_features_from_midi(file_path_ps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import file\n",
    "midi = mido.MidiFile(file_path_ps1)\n",
    "\n",
    "# initialize values\n",
    "note_counts = [0] * 128  # MIDI notes range from 0 to 127\n",
    "# total_velocity = 0\n",
    "velocities = []\n",
    "note_on_count = 0\n",
    "elapsed_time = 0\n",
    "key = '' # each file should have only 1 key. Investigate if this assumption is correct.\n",
    "tpb = midi.ticks_per_beat\n",
    "type = midi.type\n",
    "filename = os.path.basename(midi.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "second_interval = [60,90]\n",
    "print(second_interval[0])\n",
    "print(second_interval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ticks\n",
    "for msg in midi:\n",
    "    # get the key\n",
    "    if msg.is_meta and msg.type == 'key_signature':\n",
    "        key = msg.key\n",
    "    \n",
    "    # just the first n seconds\n",
    "    elapsed_time += msg.time\n",
    "    if (elapsed_time>=second_interval[0] and elapsed_time<second_interval[1]):\n",
    "        if msg.type == 'note_on' and msg.velocity > 0:\n",
    "            note_counts[msg.note] += 1\n",
    "            # total_velocity += msg.velocity\n",
    "            velocities.append(msg.velocity)\n",
    "            note_on_count += 1\n",
    "    # else:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "# Calculate velocity statistics\n",
    "if velocities:\n",
    "    average_velocity = np.mean(velocities)\n",
    "    variance_velocity = np.var(velocities)\n",
    "else:\n",
    "    average_velocity = 0\n",
    "    variance_velocity = 0\n",
    "\n",
    "# Normalize the note counts to be between 0 and 1\n",
    "normalized_note_counts = (note_counts - np.min(note_counts)) / (np.max(note_counts) - np.min(note_counts))\n",
    "\n",
    "# combine into 1 list\n",
    "combined_features = [filename, type, tpb, key, average_velocity, variance_velocity] + list(normalized_note_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = load_dataset(train_midi_path, labeled=True)\n",
    "df_labeled = create_dataframe(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_features = load_dataset(test_midi_path, labeled=False)\n",
    "df_unlabeled=create_dataframe(unlabeled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df_labeled.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the DataFrame\n",
    "print(\"\\nBasic Information about the DataFrame:\")\n",
    "print(df_labeled.info())\n",
    "\n",
    "# Generate summary statistics\n",
    "print(\"\\nSummary Statistics of the DataFrame:\")\n",
    "print(df_labeled.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in the DataFrame:\")\n",
    "print(df_labeled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm they're all type 1: \n",
    "## https://mido.readthedocs.io/en/latest/files/midi.html#file-types\n",
    "## type 1 (synchronous): all tracks start at the same time\n",
    "print(df_labeled.type.value_counts())\n",
    "print(df_unlabeled.type.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled.key.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of each numeric feature\n",
    "# numeric_columns = df_labeled.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    plt.subplot(10, 14, i+1)\n",
    "    sns.histplot(df_labeled[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of each numeric feature\n",
    "# numeric_columns = df_unlabeled.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    plt.subplot(10, 14, i+1)\n",
    "    sns.histplot(df_unlabeled[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on this, I'd remove notes 0-22, 105-127.\n",
    "The unlabeled dataset also has no info for these notes.\n",
    "See feature engineering and prep section for removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "for i, col in enumerate(numeric_columns):\n",
    "    plt.subplot(10, 14, i+1)\n",
    "    sns.histplot(df_labeled[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations between numeric features\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_labeled[numeric_columns].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between the first two numeric features and the target (if applicable)\n",
    "if 'composer' in df_labeled.columns:\n",
    "    for i in range(1,4):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x='composer', y=numeric_columns[i], data=df_labeled, hue='composer')\n",
    "        plt.title(f'{numeric_columns[i]} by Composer')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the stacked bar chart\n",
    "key_composer_counts = df_labeled.groupby(['key', 'composer']).size().unstack(fill_value=0)\n",
    "# Plot the stacked bar chart\n",
    "key_composer_counts.plot(kind='bar', stacked=True, figsize=(14, 7), colormap='viridis')\n",
    "plt.title('Number of Songs per Key, Colored by Composer')\n",
    "plt.xlabel('Key')\n",
    "plt.ylabel('Number of Songs')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(title='Composer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean + Feature engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns corresponding to notes 0-22 and 105-127\n",
    "cols_to_drop = [f'Note_{i}' for i in list(range(0, 23)) + list(range(105, 128))]\n",
    "df_labeled.drop(columns=cols_to_drop, inplace=True)\n",
    "df_unlabeled.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace Null keys with 'unk' value\n",
    "df_labeled['key']=df_labeled['key'].fillna('unk')\n",
    "df_unlabeled['key']=df_unlabeled['key'].fillna('unk')\n",
    "\n",
    "# Encode the 'key' variable\n",
    "label_encoder_key = LabelEncoder()\n",
    "df_labeled['key_encoded'] = label_encoder_key.fit_transform(df_labeled['key'])\n",
    "df_unlabeled['key_encoded'] = label_encoder_key.transform(df_unlabeled['key']) # only transform to use the same encoding as labeled\n",
    "\n",
    "# Encode the 'Composer' column\n",
    "label_encoder_composer = LabelEncoder()\n",
    "df_labeled['composer'] = label_encoder_composer.fit_transform(df_labeled['composer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features (X) and target (y)\n",
    "X = df_labeled.drop(columns=['composer', 'filename','key'])\n",
    "y = df_labeled['composer']\n",
    "\n",
    "# Define features for unlabeled data\n",
    "z = df_unlabeled.drop(columns=['filename','key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_encoder_composer.classes_,'\\n')\n",
    "print('train targets\\n',y_train.value_counts())\n",
    "print('\\ntest targets\\n',y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=10000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred_test_lr = log_reg.predict(X_test)\n",
    "y_pred_train_lr = log_reg.predict(X_train)\n",
    "# Get the classification probabilities for each class for test dataset\n",
    "y_proba_test_lr = log_reg.predict_proba(X_test)\n",
    "y_proba_train_lr = log_reg.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval(\"Logistic Regression\", y_train, y_pred_train_lr, y_proba_train_lr, y_test, y_pred_test_lr, y_proba_test_lr, label_encoder_composer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_eval(\"Logistic Regression\", y_test, y_pred_test_lr, y_proba_lr, label_encoder_composer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Random Forest classifier\n",
    "svm_classifier = LinearSVC(penalty='l2', random_state=0, tol=1e-5)\n",
    "# Wrap the LinearSVC classifier with CalibratedClassifierCV to obtain probabilities\n",
    "calibrated_svc = CalibratedClassifierCV(estimator=svm_classifier, method='sigmoid')\n",
    "calibrated_svc.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred_test_svc = calibrated_svc.predict(X_test)\n",
    "y_pred_train_svc = calibrated_svc.predict(X_train)\n",
    "# Get the classification probabilities for each class for test dataset\n",
    "y_proba_test_svc = calibrated_svc.predict_proba(X_test)\n",
    "y_proba_train_svc = calibrated_svc.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_eval(\"Support Vector Machine (CV Calibrated)\", y_test, y_pred_svc, y_proba_svc, label_encoder_composer)\n",
    "model_eval(\"Support Vector Machine (CV Calibrated)\", y_train, y_pred_train_svc, y_proba_train_svc, y_test, y_pred_test_svc, y_proba_test_svc, label_encoder_composer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred_test_rf = rf_classifier.predict(X_test)\n",
    "y_pred_train_rf = rf_classifier.predict(X_train)\n",
    "# Get the classification probabilities for each class for test dataset\n",
    "y_proba_test_rf = rf_classifier.predict_proba(X_test)\n",
    "y_proba_train_rf = rf_classifier.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_eval(\"Random Forest\", y_test, y_pred_rf, y_proba_rf, label_encoder_composer)\n",
    "model_eval(\"Random Forest\", y_train, y_pred_train_rf, y_proba_train_rf, y_test, y_pred_test_rf, y_proba_test_rf, label_encoder_composer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Random Forest classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred_test_gb = gb_classifier.predict(X_test)\n",
    "y_pred_train_gb = gb_classifier.predict(X_train)\n",
    "# Get the classification probabilities for each class for test dataset\n",
    "y_proba_test_gb = gb_classifier.predict_proba(X_test)\n",
    "y_proba_train_gb = gb_classifier.predict_proba(X_train)\n",
    "\n",
    "# # Predict the target on the test set\n",
    "# y_pred_gb = gb_classifier.predict(X_test)\n",
    "# # Get the classification probabilities for each class\n",
    "# y_proba_gb = gb_classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_eval(\"GBM\", y_test, y_pred_gb, y_proba_gb, label_encoder_composer)\n",
    "model_eval(\"GBM\", y_train, y_pred_train_gb, y_proba_train_gb, y_test, y_pred_test_gb, y_proba_test_gb, label_encoder_composer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the target on the test set\n",
    "y_pred_unlabeled = gb_classifier.predict(z)\n",
    "# Get the classification probabilities for each class\n",
    "y_proba_unlabeled = gb_classifier.predict_proba(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_unlabeled)\n",
    "print(label_encoder_composer.inverse_transform(y_pred_unlabeled))\n",
    "# print(y_proba_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for plotting\n",
    "df_plot = pd.DataFrame(y_proba_unlabeled, columns=[f'Class_{i}_prob' for i in range(y_proba_unlabeled.shape[1])])\n",
    "df_plot['Predicted_Class'] = y_pred_unlabeled\n",
    "df_plot['Predicted Composer'] = label_encoder_composer.inverse_transform(y_pred_unlabeled)\n",
    "\n",
    "# Extract the highest predicted probability for each record\n",
    "df_plot['Max_Probability'] = df_plot[[f'Class_{i}_prob' for i in range(y_proba_unlabeled.shape[1])]].max(axis=1)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(data=df_plot, x='Max_Probability', hue='Predicted Composer', multiple='stack', palette='tab10', bins=20)\n",
    "plt.title('Histogram of Predicted Probabilities by Class')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Number of Records')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this histogram, we can select a probability threshold such that\n",
    "```\n",
    "if max(predicted probability for all classes) < threshold\n",
    "then midi file is NOT one of the 4 composers in training data\n",
    "````\n",
    "To begin, I'd select a threshold of 0.90, resulting in 7 out of the 35 midi files being classified as NOT belonging to our 4 known composers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabeled[['filename']].merge(df_plot[df_plot.Max_Probability<0.90],left_index=True, right_index=True, how = 'right').drop(\n",
    "    labels=['Predicted_Class','Predicted Composer'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we would operationalize/functionalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_ps2 = test_midi_path+\"0.981087291054314_adj.mid\"\n",
    "file_path_ps1 = train_midi_path+\"Bach/Cello Suite 3_BWV1009_2217_cs3-1pre.mid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = extract_features_from_midi(file_path_ps1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
